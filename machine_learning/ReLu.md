# Rectifier

ReLU, or rectified linear unit, is an activation function.

$ f(x) = x^{+} = max(0,x) $

It is also known as: rectifier, ramp function.

## What is it? What does it do?
It is a linear function for values above zero. It sets any negative elements to 0. This allows for optimization in learning.

![ReLU function graph](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs-1024x768.png "ReLU function")

## Why is it important?
It performs a simple computation and speeds up training of neural network.